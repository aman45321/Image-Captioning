# -*- coding: utf-8 -*-
"""ImageCaptioning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RgRZf2Zx1iH-HJ5_Ckv3fhoGV2djPfyd
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from numpy import array
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import string
import os
from PIL import Image
import glob
from pickle import dump, load
from time import time
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\
                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam, RMSprop
from keras.layers.wrappers import Bidirectional
from keras.layers.merge import add
from keras.applications.inception_v3 import InceptionV3
from keras.preprocessing import image
from keras.models import Model
from keras import Input, layers
from keras import optimizers
from keras.applications.inception_v3 import preprocess_input
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

def load_doc(filename):
    # Opening file for read only
    file1 = open(filename, 'r')
    # read all text
    text = file1.read()
    # close the file
    file1.close()
    return text

filename = r"/content/drive/MyDrive/Image_Captioning/data/Flickr8k_text/Flickr8k.token.txt"
# load descriptions
doc = load_doc(filename)
print(doc[:300])



def load_description(doc):
    mapping=dict()
    for line in doc.split('\n'):
        token=line.split()
        if len(line)<2:
          continue
        image_id,image_desc=token[0],token[1:]
        image_id=image_id.split('.')[0]
        image_desc=' '.join(image_desc)
        if image_id not in mapping:
            mapping[image_id]=list()
        mapping[image_id].append(image_desc)
    return mapping

descriptions=load_description(doc)
print('Loaded: %d ' % len(descriptions))

list(descriptions.keys())[:5]

descriptions['1000268201_693b08cb0e']

#Cleaning above descriptions
def clean_descriptions(descriptions):
    table=str.maketrans('','',string.punctuation)
    for key,desc_list in descriptions.items():
      for i in range(len(desc_list)):
        desc=desc_list[i]
        desc=desc.split()
        desc=[word.lower() for word in desc]
        desc=[w.translate(table) for w in desc]
        desc=[word for word in desc if len(word)>1]
        desc=[word for word in desc if word.isalpha()]
        desc_list[i]=' '.join(desc)
    
clean_descriptions(descriptions)

descriptions['1007129816_e794419615']

# converting those loaded description into vocabulary of words
def to_vocab(descriptions):
    all_desc=set()
    for key in descriptions.keys():
        [all_desc.update(d.split()) for d in descriptions[key]]
    return all_desc

vocabulary=to_vocab(descriptions)
print('Original Vocabulary Size: %d' % len(vocabulary))

def save_descriptions(descriptions,filename):
    line=list()
    for key,desc_list in descriptions.items():
            for desc in desc_list:
                line.append(key+' '+desc)
    data='\n'.join(line)
    file1=open(filename,'w')
    file1.write(data)
    file1.close()

save_descriptions(descriptions,'descriptions.txt')

def load_doc(filename):
	# open the file as read only
	file = open(filename, 'r')
	# read all text
	text = file.read()
	# close the file
	file.close()
	return text

def load_set(filename):
    doc=load_doc(filename)
    dataset=list()
    for line in doc.split('\n'):
        if len(line)<1:
          continue
        identifier=line.split('.')[0]
        dataset.append(identifier)
    return set(dataset)

filename="/content/drive/MyDrive/Image_Captioning/data/Flickr8k_text/Flickr_8k.trainImages.txt"
train=load_set(filename)
print('Dataset: %d' % len(train))

# Below path contains all the images
images = r'/content/drive/MyDrive/Image_Captioning/data/Flicker8k_Dataset/'
# Create a list of all image names in the directory
img = glob.glob(images + '*.jpg')

#Below path contains all name of images which is to be used as train data
train_images_file = r'/content/drive/MyDrive/Image_Captioning/data/Flickr8k_text/Flickr_8k.trainImages.txt'
# Reading the train image names in a set
train_images = set(open(train_images_file, 'r').read().strip().split('\n'))

# Create a list of all the training images with their full path names
train_img = []

for i in img: # img is list of full path names of all images
    if i[len(images):] in train_images: # Check if the image belongs to training set
        train_img.append(i) # Add it to the list of train image

# Below path contains all name of images which is used to be as test data
test_images_file = r'/content/drive/MyDrive/Image_Captioning/data/Flickr8k_text/Flickr_8k.testImages.txt'
# Read the test image names in a set
test_images = set(open(test_images_file, 'r').read().strip().split('\n'))

# Create a list of all the test images with their full path names
test_img = []

for i in img: # img is list of full path names of all images
    if i[len(images):] in test_images: # Check if the image belongs to test set
        test_img.append(i) # Add it to the list of test images

def load_clean_descriptions(filename,dataset):
    doc=load_doc(filename)
    descriptions=dict()
    for line in doc.split('\n'):
        tokens=line.split()
        image_id,image_desc=tokens[0],tokens[1:]
        if image_id in dataset:
            if image_id not in descriptions:
               descriptions[image_id]=list()
            desc='statseq '+' '.join(image_desc)+' endseq'
            descriptions[image_id].append(desc)
    return descriptions

train_descriptions = load_clean_descriptions("./descriptions.txt", train)
print('Descriptions: train=%d' % len(train_descriptions))

def preprocess(img):
    img=image.load_img(img,target_size=(299,299))
    img=image.img_to_array(img)
    img=np.expand_dims(img,axis=0)
    img=preprocess_input(img)
    return img

model=InceptionV3(weights='imagenet')



model_new=Model(model.input,model.layers[-2].output)
model_new.make_predict_function()

def encode(img):
   img=preprocess(img)
   feat_vec=model_new.predict(img)
   feat_vec=np.reshape(feat_vec,feat_vec.shape[1])
   return feat_vec

# We're Calling the above funtion to encode all the train images
#start = time()
#encoding_train = {}
#for img in train_img:
 #   encoding_train[img[len(images):]] = encode(img)
#print("Time taken in seconds =", time()-start)

#with open("/content/drive/MyDrive/data/Pickle/encoded_train_images.pkl",'wb') as  encoded_pickle:
#     dump(encoding_train,encoded_pickle)

#start=time()
#encoding_test={}
#for img in test_img:
 #   encoding_test[img[len(images):]]=encode(img)
#print("Time taken in seconds =",time()-start)

#with open("/content/drive/MyDrive/data/Pickle/encoded_test_images.pkl",'wb') as encoded_pickle:
#     dump(encoding_test,encoded_pickle)

train_features=load(open("/content/drive/MyDrive/Image_Captioning/data/Pickle/encoded_train_images.pkl","rb"))
print('Photos: train=%d' % len(train_features))

all_train_captions = []
for key, val in train_descriptions.items():
    for cap in val:
        all_train_captions.append(cap)
len(all_train_captions)

word_count_thresold=10
word_counts={}
n_sent=0
for sent in all_train_captions:
    n_sent+=1
    for w in sent.split(' '):
       word_counts[w]=word_counts.get(w,0)+1
vocab=[w for w in word_counts if word_counts[w]>=word_count_thresold]
print('preprocessed words %d -> %d' % (len(word_counts), len(vocab)))

wordtoix={}
ixtoword={}
ix=1
for w in vocab:
   wordtoix[w]=ix
   ixtoword[ix]=w
   ix+=1

vocab_size=len(ixtoword)+1
vocab_size

def to_lines(descriptions):
    all_desc=list()
    for key in descriptions.keys():
        [all_desc.append(d) for d in descriptions[key]]
    return all_desc

def max_length(descriptions):
    lines=to_lines(descriptions)
    return max(len(d.split()) for d in lines)

max_length = max_length(train_descriptions)
print('Description Length: %d' % max_length)

from tensorflow.keras.utils import to_categorical

def data_generator(descriptions,photos,wordtoix,max_length,num_photos_per_batch):
      X1,X2,y=list(),list(),list()
      n=0
      while 1:
        for key,desc_list in descriptions.items():
            n+=1
            photo=photos[key+'.jpg']
            for desc in desc_list:
                seq=[wordtoix[word] for word in doc.split(' ') if word in wordtoix]

                for i in range(1,len(seq)):
                  in_seq,out_seq=seq[:i],seq[i]
                  in_seq=pad_sequences([in_seq],maxlen=max_length)[0]
                  out_seq=to_categorical([out_seq],num_classes=vocab_size)[0]
                  X1.append(photo)
                  X2.append(in_seq)
                  y.append(out_seq)
            if n==num_photos_per_batch:
                yield [[array(X1), array(X2)], array(y)]
                X1, X2, y = list(), list(), list()
                n=0

glove_dir="/content/drive/MyDrive/glove"
embedding_index={}
f=open(os.path.join(glove_dir,'glove.6B.200d.txt'),encoding="utf-8")
for line in f:
  value=line.split()
  word=value[0]
  coffs=np.asarray(value[1:],dtype='float32')
  embedding_index[word]=coffs
f.close()
print('Found %s word vectors.' % len(embedding_index))

embedding_dim = 200

# Get 200-dimension dense vector for each of the 10000 words in out vocabulary
embedding_matrix = np.zeros((vocab_size, embedding_dim))

for word, i in wordtoix.items():
    #if i < max_words:
    embedding_vector = embedding_index.get(word)
    if embedding_vector is not None:
        # Words not found in the embedding index will be all zeros
        embedding_matrix[i] = embedding_vector

embedding_matrix.shape

inputs1 = Input(shape=(2048,))
fe1 = Dropout(0.5)(inputs1)
fe2 = Dense(256, activation='relu')(fe1)
inputs2 = Input(shape=(max_length,))
se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)
se2 = Dropout(0.5)(se1)
se3 = LSTM(256)(se2)
decoder1 = add([fe2, se3])
decoder2 = Dense(256, activation='relu')(decoder1)
outputs = Dense(vocab_size, activation='softmax')(decoder2)
model = Model(inputs=[inputs1, inputs2], outputs=outputs)

model.summary()

#We set weights for layers here
model.layers[2].set_weights([embedding_matrix])
model.layers[2].trainable = False

model.compile(loss='categorical_crossentropy',optimizer='adam')

epochs = 20
number_pics_per_batch = 3
steps = len(train_descriptions)//number_pics_per_batch

'''for i in range(epochs):
   generator=data_generator(train_descriptions,train_features,wordtoix,max_length,number_pics_per_batch)
   model.fit_generator(generator,epochs=1,steps_per_epoch=steps,verbose=1)
   model.save('./modelweights/model_' +str(i) + '.h5')'''

model.load_weights('/content/drive/MyDrive/modelweights/model_weights/model_49.h5')

images='/content/drive/MyDrive/Image_Captioning/data/Flicker8k_Dataset/'

with open("/content/drive/MyDrive/Image_Captioning/data/Pickle/encoded_test_images.pkl", "rb") as encoded_pickle:
    encoding_test = load(encoded_pickle)

def imageSearch(photo):
    in_text = 'startseq'
    for i in range(max_length):
        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]
        sequence = pad_sequences([sequence], maxlen=max_length)
        yhat = model.predict([photo,sequence], verbose=0)
        yhat = np.argmax(yhat)
        word = ixtoword[yhat]
        in_text += ' ' + word
        if word == 'endseq':
            break
    final = in_text.split()
    final = final[1:-1]
    final = ' '.join(final)
    return final

'''pic = list(encoding_test.keys())[550]
image = encoding_test[pic].reshape((1,2048))
x=plt.imread(images+pic)
plt.imshow(x)
plt.show()
print("Image with Caption:",imageSearch(image))'''

def caption_this_image(input_img):
    enc=encode(input_img).reshape(1,2048)

    caption=imageSearch(enc)
    return caption











